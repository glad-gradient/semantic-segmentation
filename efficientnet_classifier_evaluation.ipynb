{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pandas import read_csv, merge, concat\n",
    "from skimage.io import imread\n",
    "from cv2 import resize\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "\n",
    "from efficientnet.keras import EfficientNetB0\n",
    "from efficientnet.keras import preprocess_input\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "STEPS_PER_EPOCH = 2000\n",
    "RANDOM_SEED = 10000\n",
    "IMG_SIZE = 256\n",
    "\n",
    "PATH_CHECKPOINT = 'models'\n",
    "MODEL = 'efficientnet_classifier'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype=np.int16)\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return all_masks\n",
    "\n",
    "\n",
    "def make_image_generator(in_df, batch_size=BATCH_SIZE):\n",
    "    all_batches = list(in_df.groupby('ImageId'))\n",
    "    out_rgb = []\n",
    "    labels = []\n",
    "    while True:\n",
    "        np.random.shuffle(all_batches)\n",
    "        for c_img_id, c_masks in all_batches:\n",
    "            rgb_path = os.path.join(train_image_dir, c_img_id)\n",
    "            _img = imread(rgb_path)\n",
    "            _img = resize(_img, (IMG_SIZE, IMG_SIZE))\n",
    "            _img = _img.astype(np.float32)\n",
    "\n",
    "            out_rgb += [_img]\n",
    "            labels.append(c_masks['has_ship'].iloc[0])\n",
    "            if len(out_rgb) >= batch_size:\n",
    "                yield np.stack(out_rgb, 0), np.array(labels)\n",
    "                out_rgb, labels = [], []\n",
    "\n",
    "\n",
    "def create_aug_generator(in_gen, _image_generator, seed=RANDOM_SEED):\n",
    "    np.random.seed(seed)\n",
    "    for in_x, in_y in in_gen:\n",
    "        _seed = np.random.choice(range(seed))\n",
    "        sample = _image_generator.flow(in_x,\n",
    "                                       in_y,\n",
    "                                       batch_size=in_x.shape[0],\n",
    "                                       seed=_seed,\n",
    "                                       shuffle=True)\n",
    "\n",
    "        yield next(sample)\n",
    "\n",
    "\n",
    "def preprocess_data(_train_image_dir, _exclude_list, n_train=0.8, balanced=False):\n",
    "    masks = read_csv(os.path.join(data_dir, 'train_ship_segmentations_v2.csv.zip'))\n",
    "\n",
    "    masks = masks[~masks['ImageId'].isin(_exclude_list)]\n",
    "\n",
    "    masks['ships'] = masks['EncodedPixels'].map(lambda row: 1 if isinstance(row, str) else 0)\n",
    "    unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "\n",
    "    unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x > 0 else 0.0)\n",
    "\n",
    "    n_train_split = int(n_train * len(unique_img_ids))\n",
    "\n",
    "    train_ids = unique_img_ids.iloc[:n_train_split]\n",
    "    valid_ids = unique_img_ids.iloc[n_train_split:]\n",
    "\n",
    "    if balanced:\n",
    "        ones = train_ids.loc[train_ids['has_ship'] == 1]\n",
    "        zeros = train_ids.loc[train_ids['has_ship'] == 0]\n",
    "        zeros = zeros.sample(len(ones))\n",
    "        train_ids = concat([ones, zeros])\n",
    "\n",
    "    masks.drop(['ships'], axis=1, inplace=True)\n",
    "\n",
    "    _train_df = merge(masks, train_ids)\n",
    "    _valid_df = merge(masks, valid_ids)\n",
    "\n",
    "    return _train_df, _valid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = ['6384c3e78.jpg', '13703f040.jpg', '14715c06d.jpg', '33e0ff2d5.jpg',\n",
    "                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg',\n",
    "                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n",
    "                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg',\n",
    "                '66049e4ea.jpg']  # corrupted images\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "data_dir = configs[\"DATA_DIRECTORY\"]\n",
    "train_image_dir = os.path.join(data_dir, 'train_v2')\n",
    "\n",
    "train_df, test_df = preprocess_data(train_image_dir, exclude_list, n_train=0.85, balanced=True)    \n",
    "\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "model = load_model(f'models/{MODEL}.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_generator = create_aug_generator(make_image_generator(train_df), data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31314143538475037, 0.9286249876022339]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(train_eval_generator, steps=STEPS_PER_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_generator = create_aug_generator(make_image_generator(test_df), data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.006274908781051636, 0.9441249966621399]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_eval_generator, steps=STEPS_PER_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
